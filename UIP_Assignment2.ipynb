{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "UIP_Assignment2",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thenerdyouknow/AML_Final_Project/blob/master/UIP_Assignment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ay4zJHdy0wQe",
        "colab_type": "text"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "This is a Python notebook which contains code that can be used to build language models based on corpora of texts. To run this notebook you need to follow the following steps :\n",
        "\n",
        "1. Download your corpora(for example, mine are books from Project Gutenberg).\n",
        "2. Zip them and upload them to your Google Drive.\n",
        "3. Mount your drive on Google Colaboratory.\n",
        "4. Copy and paste the paths of the file under FILE_1 and FILE_2(you can create more global variables if you'd like).\n",
        "5. Run all the cells in order.\n",
        "\n",
        "The files I used are available as a zip, if you upload the zip as is on Google Drive and run the code on Google Colaboratory, it should probably run without any errors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dj38OlXqPCM2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Importing all the necessary tools. Since the idea was to build a language model from scratch, minimal libraries have been used.\n",
        "import string\n",
        "import re\n",
        "from collections import Counter\n",
        "from itertools import islice"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CInneW8wDQRB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Mounting Google Drive files and unzipping the datasets' zip.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!unzip \"/content/drive/My Drive/Datasets.zip\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QE9ElxiOY0S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Global variables for the files\n",
        "FILE_1 = '/content/Datasets/TaleOfTwoCities.txt'\n",
        "FILE_2 = '/content/Datasets/WarAndPeace.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPnF5RQ7_T84",
        "colab_type": "text"
      },
      "source": [
        "# Section 1(Cleaning and N-Gram Creation):\n",
        "\n",
        "This section contains functions to clean the dataset, create the n-grams, and store them in an appropriate datastructure so we can use it for our language models.\n",
        "\n",
        "Contains : \n",
        "\n",
        "1.   open_and_read()\n",
        "2.   generate_ngrams()\n",
        "3.   split_ngrams()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRw2aBGvyK0v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def open_and_read(filepath):\n",
        "  '''\n",
        "  def open_and_read():\n",
        "  Input: the path of the file\n",
        "  Output: A cleaned string(removing all punctuations, other problematic elements, \n",
        "          adding start and end of line, etc. etc.)\n",
        "  \n",
        "  In this function, the following happens(in order):\n",
        "  1. File contents are read line by line.\n",
        "  2. A loop runs over all lines which does the following:\n",
        "       a. Converts all the characters to lower case.\n",
        "       b. Deals with all the words with a period sign in front of them(as periods will be considered\n",
        "          as End of Line so these words will cause issues later).\n",
        "       c. All period signs are converted to <s> </s> where <s> is start of sentence and </s>\\\n",
        "          is end of sentence.\n",
        "       d. If length of the string is more than 4, we check if the string has a closing quotations\n",
        "          and a \\n character after as examination of the corpus revealed that it was usually\n",
        "          end of sentence.\n",
        "       e. Replaces all the \\n new lines with space.\n",
        "       f. Removes all punctuation using str.translate.\n",
        "       g. Append the cleaned string to new list.\n",
        "  3. Convert the cleaned list to a string and return it.\n",
        "  '''\n",
        "  final_list = []\n",
        "  with open(filepath,'rt') as open_book:\n",
        "    file_contents = open_book.readlines()\n",
        "  for each_string in file_contents:\n",
        "    each_string = each_string.lower()\n",
        "    final_string = each_string.replace(\"st.\", \"\")\n",
        "    final_string = final_string.replace(\"mrs.\",\"\")\n",
        "    final_string = final_string.replace(\"mr.\",\"\")\n",
        "    final_string = final_string.replace(\".\",\" </s> <s> \")\n",
        "    if(len(each_string)>4):\n",
        "      if(each_string[-2]==\"”\" and each_string[-3]!=\",\"):\n",
        "        final_string = final_string + \" </s> <s> \"\n",
        "    final_string = final_string.replace(\"\\n\", \" \")\n",
        "    final_string = final_string.translate(str.maketrans('','',punctuation))\n",
        "    final_list.append(final_string)\n",
        "  return ''.join(final_list)\n",
        "\n",
        "def generate_ngrams(s, n):\n",
        "  '''\n",
        "  def generate_ngrams():\n",
        "  Input: The string to split into n-grams, and the length of the n-grams that need to be formed.\n",
        "  Output: List of all n-grams in the string\n",
        "  \n",
        "  This is a clever function. It does the following:\n",
        "  1. Tokenizes the string on spaces.\n",
        "  2. Splits the string into n pieces(with each list being one-off the previous one),\n",
        "     and uses zip to combine the n-th element of each piece together. \n",
        "     \n",
        "     If one string is bigger than the other, the extra elements are discarded.\n",
        "     For example:\n",
        "     'i had a little lamb haha' with a bi-gram becomes\n",
        "     splits to 'i had a little lamb haha', 'had a little lamb haha'\n",
        "     the n-grams become :\n",
        "     [('i','had'),('had','a'),('a','little'),('little','lamb'),('lamb','haha')]\n",
        "  3. Joins the strings together, puts them in a list and returns the list\n",
        "  '''\n",
        "  tokens = [token for token in s.split(\" \") if token != \"\"]\n",
        "  ngrams = zip(*[tokens[i:] for i in range(n)])\n",
        "  return [\" \".join(ngram) for ngram in ngrams]\n",
        "  \n",
        "def split_ngrams(ngrams):\n",
        "  '''\n",
        "  def split_ngrams():\n",
        "  Input: The n-gram list\n",
        "  Output: A dictionary, the keys of which are the all the words of an n-gram except\n",
        "          the last one, and the values being a dictionary of all the words that\n",
        "          come after the words in the key.\n",
        "          For example: if the n-gram is 'you are the best', 'you are the worst'\n",
        "          The dict will look like this : {'you are the':{'best':1,'worst':1}}\n",
        "  \n",
        "  Another clever function. It does the following:\n",
        "  1. Loops over all the n-grams.\n",
        "  2. For each n-gram it does the following:\n",
        "        a. Check if there is already a dictionary element for the words from\n",
        "           position 0 to n-1(also indexed by -1 in Python).\n",
        "           If yes, append the word that appears after the words from 0 to n-1.\n",
        "        b. If not, create a list with the word that appears after words 0 to n-1.\n",
        "  3. Then, for each ngram, it does the following:\n",
        "        a. Adds up all the frequencies, converts to list and sorts it, then \n",
        "           converts it back to a dict.\n",
        "        b. Sets the dictionary in place of the list for that n-gram.     \n",
        "  4. Returns the dictionary of all the n-grams.\n",
        "  '''\n",
        "  ngrams_dict = {}\n",
        "  \n",
        "  for each_ngram in ngrams:\n",
        "    try:\n",
        "      ngrams_dict[' '.join(each_ngram.split(\" \")[:-1])].append(each_ngram.split(\" \")[-1])\n",
        "    except KeyError:\n",
        "      ngrams_dict[' '.join(each_ngram.split(\" \")[:-1])] = [each_ngram.split(\" \")[-1]]\n",
        "\n",
        "  for ngram, value in ngrams_dict.items():\n",
        "    new_value = dict(Counter(value).most_common())\n",
        "    ngrams_dict[ngram] = new_value \n",
        "  \n",
        "  return ngrams_dict\n",
        "  \n",
        "punctuation = \"“”!\\\"#$%&'()*+,-.:;=?@[\\]^_`{|}~\"\n",
        "\n",
        "file_1 = open_and_read(FILE_1)\n",
        "file_2 = open_and_read(FILE_2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RILLhgj-_0Xc",
        "colab_type": "text"
      },
      "source": [
        "# Section 2(Language Models):\n",
        "\n",
        "Contains the functions that are used to build the language models and predict the end of sentences using the beginning of the sentences given in global variables. File mappings are also stored in a global variable.\n",
        "\n",
        "Contains:\n",
        "\n",
        "1.   take()\n",
        "2.   one_gram_prediction()\n",
        "3.   all_gram_prediction()\n",
        "4.   final_predictions()\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGnZjeyla5bm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Bunch of global variables like how big should the predicted sentence be, the beginning of the sentences, etc. etc.\n",
        "SENTENCE_LENGTH = 20\n",
        "list_of_sentences = ['i suppose','and having got','not two minutes']\n",
        "files_mapping = {1:file_1,2:file_2}\n",
        "\n",
        "\n",
        "def take(n, iterable_object):\n",
        "  '''\n",
        "  def take():\n",
        "  Input: Value of n and an iterable object.\n",
        "  Output: Just n values of all the iterable objects\n",
        "\n",
        "  Standard function. Used to cut the dictionary to the length of just all the\n",
        "  words needed to complete the sentence for the mono-gram function.\n",
        "  '''\n",
        "  return list(islice(iterable_object, n))\n",
        "\n",
        "def one_gram_prediction(sentence,ngrams):\n",
        "  '''\n",
        "  def one_gram_prediction():\n",
        "  Input: The sentence and the n-grams dictionary\n",
        "  Output: The predicted sentence using a mono-gram language model.\n",
        "  \n",
        "  The function does the following:\n",
        "  1. Cuts the dictionary to just the words needed to finish the sentence\n",
        "     and make it _atleast_ 10 words long.\n",
        "  2. Loops through the edited list and just appends to the end of the sentence.\n",
        "     This works because the dict elements were\n",
        "     inserted in a descending order, so the first few words are the ones that\n",
        "     occur the most in the dataset.\n",
        "  3. Returns predicted sentence.\n",
        "  '''\n",
        "  n_ngrams = take((SENTENCE_LENGTH+2) - len(sentence.split(\" \")), ngrams.items())\n",
        "  for keys in n_ngrams:\n",
        "    if(keys[0] != '</s>' and keys[0] != '<s>'):\n",
        "      sentence = sentence + ' ' + keys[0]\n",
        "  return sentence\n",
        "\n",
        "\n",
        "def all_gram_prediction(sentence,ngrams,ngram_value):\n",
        "  '''\n",
        "  def all_gram_prediction():\n",
        "  Input: The beginning sentence, the n-grams, and the n value in the n-grams.\n",
        "  Output: The predicted sentence\n",
        "  \n",
        "  Another clever function(I clearly think all my functions are clever). It does the following:\n",
        "  1. Splits the beginning sentence on space.\n",
        "  2. Runs a while loop while the length of the predicted sentence is smaller than the desired length. It does this in the loop:\n",
        "        a. Split the sentence again, this is done because it needs to update.\n",
        "        b. Checks if ngram_value has a value of 1, if yes, then redirects to \n",
        "           the mono-gram function to find a predicted sentence using monograms\n",
        "           and returns that sentence.\n",
        "        c. Checks if the ngram_value has a value of 2, if yes, then it just\n",
        "           takes the previous word for context as that's the protocol for a\n",
        "           bi-gram model.\n",
        "        d. Checks if the ngram_value has a value of 3, if yes, then it takes\n",
        "           the last two words for context as that's the protocol for a tri-gram\n",
        "           model.\n",
        "        e. Similarly, for ngram_value of 4, it takes the last three words as \n",
        "           context because 4 indicates quad-gram model.\n",
        "        f. If the last word of the context is the start token(<s>) then it \n",
        "           deletes the end token value from the n-grams, as this will\n",
        "           ensure the predicted sentence is not stuck in a loop of <s></s><s></s>...\n",
        "        g. Tries to find the word most used after the word(s) that are the \n",
        "           context, if it doesn't find anything then it doesn't exist in the \n",
        "           n-grams so it just initializes the word to a blank.\n",
        "        h. Adds the new word to the sentence and returns to the start of the\n",
        "           loop with the new sentence\n",
        "   3. Returns the predicted sentence.\n",
        "  '''\n",
        "  split_sentence = sentence.split(\" \")\n",
        "  \n",
        "  while(len(split_sentence)<=SENTENCE_LENGTH):\n",
        "    \n",
        "    split_sentence = sentence.split(\" \")\n",
        "    \n",
        "    if(ngram_value == 1):\n",
        "      final_sentence = one_gram_prediction(sentence,ngrams)\n",
        "      return final_sentence\n",
        "    \n",
        "    elif(ngram_value == 2):\n",
        "      words_needed = split_sentence[-1]\n",
        "    \n",
        "    elif(ngram_value == 3):\n",
        "      words_needed = split_sentence[-2] + ' '+ split_sentence[-1]\n",
        "    \n",
        "    elif(ngram_value == 4):\n",
        "      words_needed = split_sentence[-3] + ' ' + split_sentence[-2] + ' '+ split_sentence[-1]\n",
        "      \n",
        "    if(split_sentence[-1] == '<s>'):\n",
        "      try:\n",
        "        del ngrams[words_needed]['</s>']\n",
        "      except:\n",
        "        pass\n",
        "      \n",
        "    try:\n",
        "      new_word = list(ngrams[words_needed])[0]\n",
        "    except:\n",
        "      new_word = ''\n",
        "      \n",
        "    sentence = sentence + ' ' + new_word\n",
        "    \n",
        "  return sentence\n",
        "\n",
        "\n",
        "def final_predictions(ngram_value):\n",
        "  '''\n",
        "  def final_predictions():\n",
        "  Input: value of the n in the n-grams needed.\n",
        "  Output: All the predicted sentences as found in the list of global variables, using the length given in the global variables.\n",
        "  \n",
        "  Driver function just written for ease of use when predicting words. It does the following:\n",
        "  1. Loops through all the files in the global file mapping dictionary.\n",
        "  2. Generates n-grams for the file.\n",
        "  3. If the n-gram value is 1, then generates frequency from the n-grams for\n",
        "     mono-gram prediction.\n",
        "  4. Else, just uses the split_ngrams function to get the n-gram dictionary.\n",
        "  5. Loops through all the beginning sentences, calls all_gram_prediction to get\n",
        "     the predicted sentences and appends them to a list.\n",
        "  6. Returns the list of all predicted sentences\n",
        "  '''\n",
        "  sentence_list = []\n",
        "  for i in range(1,3):\n",
        "    ngrams = generate_ngrams(files_mapping[i],ngram_value)\n",
        "    \n",
        "    if(ngram_value == 1):\n",
        "      ngrams = dict(Counter(ngrams).most_common())\n",
        "    else:  \n",
        "      ngrams = split_ngrams(ngrams)\n",
        "      \n",
        "    for each_sentence in list_of_sentences:\n",
        "      each_sentence = '<s> ' + each_sentence\n",
        "      sentence = all_gram_prediction(each_sentence,ngrams,ngram_value)\n",
        "      sentence_list.append(sentence)\n",
        "      \n",
        "  return sentence_list\n",
        "\n",
        "\n",
        "final_sentences = final_predictions(4)#Example for quad-gram.\n",
        "print(final_sentences)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}